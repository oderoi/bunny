{
  "name": "bunny-ai",
  "version": "0.1.3",
  "description": "Local LLM runner using native llama.cpp and Hugging Face GGUF models",
  "main": "src/bunny/cli.py",
  "homepage": "https://github.com/your-repo/bunny",
  "repository": {
    "type": "git",
    "url": "https://github.com/your-repo/bunny.git"
  },
  "keywords": [
    "ai",
    "llm",
    "llama",
    "local",
    "offline",
    "chat",
    "mobile",
    "cross-platform"
  ],
  "author": "Your Name <your.email@example.com>",
  "license": "MIT",
  "engines": {
    "node": ">=16.0.0",
    "python": ">=3.9"
  },
  "scripts": {
    "install": "python install.py",
    "install-mobile": "bash install-mobile.sh",
    "install-docker": "docker-compose up -d",
    "build": "python -m build",
    "test": "python -m pytest",
    "lint": "python -m flake8 src/",
    "format": "python -m black src/",
    "ui:dev": "cd web/ui && npm run dev",
    "ui:build": "cd web/ui && npm run build",
    "ui:preview": "cd web/ui && npm run preview"
  },
  "dependencies": {
    "huggingface-hub": ">=0.20.0",
    "click": ">=8.1.0",
    "requests": ">=2.28.0",
    "fastapi": ">=0.100.0",
    "uvicorn": ">=0.20.0",
    "python-multipart": ">=0.0.6"
  },
  "devDependencies": {
    "pytest": ">=7.0.0",
    "flake8": ">=6.0.0",
    "black": ">=23.0.0",
    "mypy": ">=1.0.0"
  },
  "files": [
    "src/",
    "web/",
    "install.py",
    "install.sh",
    "install.bat",
    "install-mobile.sh",
    "Dockerfile",
    "docker-compose.yml",
    "README.md",
    "INSTALL.md",
    "LICENSE"
  ],
  "bin": {
    "bunny": "src/bunny/cli.py",
    "b": "src/bunny/cli.py"
  },
  "os": [
    "darwin",
    "linux",
    "win32"
  ],
  "cpu": [
    "x64",
    "arm64"
  ],
  "preferGlobal": true,
  "publishConfig": {
    "access": "public"
  }
}
